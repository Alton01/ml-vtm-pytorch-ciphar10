{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOi7hvyLbM/6/SRwYV9kuhs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alton01/ml-vtm-pytorch-ciphar10/blob/main/cifar_10VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ouq7o88fPDTt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4dNCITNJRspE",
        "outputId": "9a27f061-6a15-4846-b13b-9d24c74f7d77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8lLjdvdRR8hA",
        "outputId": "54b80042-47b1-49b0-bf8d-5ec36f2b8bd8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.21.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. setup device agnostic code"
      ],
      "metadata": {
        "id": "ZixSqpMPSVDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMf9pCBoS_fE",
        "outputId": "368b63c1-0eb3-4af1-954d-cf9e139cc577"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xWzikhc9ScAP",
        "outputId": "53e32e62-039f-404c-f179-da57507a0f1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Using Device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlXxjqC8ULv3",
        "outputId": "9003958a-5de6-49a8-a7f0-fa58a8bfe2c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Set the seed"
      ],
      "metadata": {
        "id": "2-TfVOX0UVHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "aRwG5IQKUe1z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3e-4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUw5PsxxVnQQ",
        "outputId": "4b7de7e7-6033-44cc-9674-7272290b849c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Setting the hyperparameters"
      ],
      "metadata": {
        "id": "MRnaX-IcU8zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 3e-4\n",
        "PATCH_SIZE = 4\n",
        "NUM_CLASSES = 10\n",
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "EMBED_DIM = 256\n",
        "NUM_HEADS = 8\n",
        "DEPTH = 6\n",
        "MLP_DIM = 512\n",
        "DROP_RATE = 0.1"
      ],
      "metadata": {
        "id": "pp0hwg78VIKv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 5.Define image transformations operation"
      ],
      "metadata": {
        "id": "GLJxjpGaW88h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "    #helps the model to converge faster\n",
        "    # helps to make the numerical computation stable\n",
        "])"
      ],
      "metadata": {
        "id": "9FSs0lBzXGXa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6. Getting a dataset"
      ],
      "metadata": {
        "id": "sRd71HkaYN7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root=\"data\",\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJDeB54wYX7W",
        "outputId": "9f817a5d-932e-4f7a-b4bd-aad41aa49730"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:12<00:00, 13.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.CIFAR10(root=\"data\",\n",
        "                                train=False,\n",
        "                                download=True,\n",
        "                                transform=transform)"
      ],
      "metadata": {
        "id": "7PZPOtq_gJXN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0QkATx1gklm",
        "outputId": "dc691d57-ae3e-4bd7-a910-629664799254"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=0.5, std=0.5)\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9GV9ePkgwbi",
        "outputId": "b8292b49-0b40-43f0-9011-313642e68071"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 10000\n",
              "    Root location: data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=0.5, std=0.5)\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## step 7 converting our datasets into dataloaders\n",
        "## This means breaking dataset into batches or minibatches\n",
        "## Computationally efficient ascomputing hardware may not handle 50,000 images\n",
        "## At a go\n",
        "## so we break it into 128 images at a time (BATCH_SIZE = 128)\n",
        "## It gives our neural network more chances to update its gradient per epoch"
      ],
      "metadata": {
        "id": "Kf7pEZ2ag912"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=False)"
      ],
      "metadata": {
        "id": "-Z73ow7ZiDVy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets check out what we have created\n",
        "print(f\"DataLoader: {train_loader, test_loader}\")\n",
        "print(f\"Length of train_loader: {len(train_loader)} batches of {BATCH_SIZE}...\")\n",
        "print(f\"Length of test_loader: {len(test_loader)} batches of {BATCH_SIZE}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_4YG462jxhz",
        "outputId": "7b5b8a41-b8a5-4b96-87ab-70270091e033"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoader: (<torch.utils.data.dataloader.DataLoader object at 0x7d907e670450>, <torch.utils.data.dataloader.DataLoader object at 0x7d8faf099750>)\n",
            "Length of train_loader: 391 batches of 128...\n",
            "Length of test_loader: 79 batches of 128...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 8. Building Vision Transformer model from scratch.\n",
        "#Splitting each image into smaller bits(patches)"
      ],
      "metadata": {
        "id": "rCzMePiilEK9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size,\n",
        "               patch_size,\n",
        "               in_channels,\n",
        "               embed_dim):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.proj = nn.Conv2d(in_channels=in_channels,\n",
        "                          out_channels=embed_dim,\n",
        "                          kernel_size=patch_size,\n",
        "                          stride=patch_size)\n",
        "    num_patches = (img_size // patch_size) ** 2\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches), embed_dim)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    B = x.size(0)\n",
        "    x = self.proj(x)\n",
        "    x = x.flatten(2).transpose(1, 2)\n",
        "    cls_token = self.cls_token(B, -1, -1)\n",
        "    x = torch.cat((cls_token, x), dim=1)\n",
        "    x = x + self.pos_embed\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "A3YGZIN-lOE9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define multi layer perceptron class"
      ],
      "metadata": {
        "id": "4DbPR77VrRDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_features,\n",
        "               hidden_features,\n",
        "               drop_rate):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features=in_features,\n",
        "                         out_features=hidden_features)\n",
        "    self.fc2 = nn.Linear(in_features=hidden_features,\n",
        "                         out_features=in_features)\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(F.gelu(self.fc1(x)))\n",
        "    x = self.dropout(self.fc2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "Rn-UE_FHrWXi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Building a transformer encoder layer"
      ],
      "metadata": {
        "id": "b_Fd3bZ7xLw8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, mlp_dim, drop_rate):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "yngDwS5cxQOK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create a vision transformer class to combine all of these components"
      ],
      "metadata": {
        "id": "mjPY_JlB1jDj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__ (self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, drop_rate):\n",
        "    super().__init__()\n",
        "    self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "    self.encoder = nn.Sequential([\n",
        "        TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)\n",
        "        for _ in range(depth)\n",
        "    ])\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_embed(x)\n",
        "    x = self.encoder(x)\n",
        "    x = self.norm(x)\n",
        "    cls_token = x[:, 0]\n",
        "    return self.head(cls_token)"
      ],
      "metadata": {
        "id": "BnOJCYqr1rdw"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}