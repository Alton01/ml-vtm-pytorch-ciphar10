{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO6FLBOVRluW+ssd/D4d/OS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alton01/ml-vtm-pytorch-ciphar10/blob/main/cifar_10VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ouq7o88fPDTt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4dNCITNJRspE",
        "outputId": "3738edfe-cc45-443c-89b0-5fe421ba8895"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8lLjdvdRR8hA",
        "outputId": "c80b565b-fbcd-4331-e3f5-6567d54968e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.21.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. setup device agnostic code"
      ],
      "metadata": {
        "id": "ZixSqpMPSVDI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMf9pCBoS_fE",
        "outputId": "ceddb6d2-a2da-4484-fadf-d8639e03ef94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xWzikhc9ScAP",
        "outputId": "2874df65-5a2b-4642-8523-5a74aace6653"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Using Device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlXxjqC8ULv3",
        "outputId": "29ebce03-7d6c-4715-ba2a-6479cfc1ee1f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Set the seed"
      ],
      "metadata": {
        "id": "2-TfVOX0UVHf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "aRwG5IQKUe1z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3e-4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUw5PsxxVnQQ",
        "outputId": "4b7de7e7-6033-44cc-9674-7272290b849c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Setting the hyperparameters"
      ],
      "metadata": {
        "id": "MRnaX-IcU8zM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 3e-4\n",
        "PATCH_SIZE = 4\n",
        "NUM_CLASSES = 10\n",
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "EMBED_DIM = 256\n",
        "NUM_HEADS = 8\n",
        "DEPTH = 6\n",
        "MLP_DIM = 512\n",
        "DROP_RATE = 0.1"
      ],
      "metadata": {
        "id": "pp0hwg78VIKv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 5.Define image transformations operation"
      ],
      "metadata": {
        "id": "GLJxjpGaW88h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "    #helps the model to converge faster\n",
        "    # helps to make the numerical computation stable\n",
        "])"
      ],
      "metadata": {
        "id": "9FSs0lBzXGXa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6. Getting a dataset"
      ],
      "metadata": {
        "id": "sRd71HkaYN7L"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root=\"data\",\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJDeB54wYX7W",
        "outputId": "230ebd32-bf6b-4015-91be-711b90ab59c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 39.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.CIFAR10(root=\"data\",\n",
        "                                train=False,\n",
        "                                download=True,\n",
        "                                transform=transform)"
      ],
      "metadata": {
        "id": "7PZPOtq_gJXN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0QkATx1gklm",
        "outputId": "e8ee0c69-7f4b-4d3a-d523-3b550dc68066"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=0.5, std=0.5)\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9GV9ePkgwbi",
        "outputId": "12038880-183c-4ef8-d176-c39b58ed6b69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 10000\n",
              "    Root location: data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=0.5, std=0.5)\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## step 7 converting our datasets into dataloaders\n",
        "## This means breaking dataset into batches or minibatches\n",
        "## Computationally efficient ascomputing hardware may not handle 50,000 images\n",
        "## At a go\n",
        "## so we break it into 128 images at a time (BATCH_SIZE = 128)\n",
        "## It gives our neural network more chances to update its gradient per epoch"
      ],
      "metadata": {
        "id": "Kf7pEZ2ag912"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=False)"
      ],
      "metadata": {
        "id": "-Z73ow7ZiDVy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets check out what we have created\n",
        "print(f\"DataLoader: {train_loader, test_loader}\")\n",
        "print(f\"Length of train_loader: {len(train_loader)} batches of {BATCH_SIZE}...\")\n",
        "print(f\"Length of test_loader: {len(test_loader)} batches of {BATCH_SIZE}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_4YG462jxhz",
        "outputId": "f206753f-4fb4-45f9-f874-f8c00f2388da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoader: (<torch.utils.data.dataloader.DataLoader object at 0x7fcc4ebade50>, <torch.utils.data.dataloader.DataLoader object at 0x7fcc575cbed0>)\n",
            "Length of train_loader: 391 batches of 128...\n",
            "Length of test_loader: 79 batches of 128...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 8. Building Vision Transformer model from scratch.\n",
        "#Splitting each image into smaller bits(patches)"
      ],
      "metadata": {
        "id": "rCzMePiilEK9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size,\n",
        "               patch_size,\n",
        "               in_channels,\n",
        "               embed_dim):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.proj = nn.Conv2d(in_channels=in_channels,\n",
        "                          out_channels=embed_dim,\n",
        "                          kernel_size=patch_size,\n",
        "                          stride=patch_size)\n",
        "    num_patches = (img_size // patch_size) ** 2\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    B = x.size(0)\n",
        "    x = self.proj(x)\n",
        "    x = x.flatten(2).transpose(1, 2)\n",
        "    cls_token = self.cls_token(B, -1, -1)\n",
        "    x = torch.cat((cls_token, x), dim=1)\n",
        "    x = x + self.pos_embed\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "A3YGZIN-lOE9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define multi layer perceptron class"
      ],
      "metadata": {
        "id": "4DbPR77VrRDy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_features,\n",
        "               hidden_features,\n",
        "               drop_rate):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features=in_features,\n",
        "                         out_features=hidden_features)\n",
        "    self.fc2 = nn.Linear(in_features=hidden_features,\n",
        "                         out_features=in_features)\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(F.gelu(self.fc1(x)))\n",
        "    x = self.dropout(self.fc2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "Rn-UE_FHrWXi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Building a transformer encoder layer"
      ],
      "metadata": {
        "id": "b_Fd3bZ7xLw8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, mlp_dim, drop_rate):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "yngDwS5cxQOK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create a vision transformer class to combine all of these components"
      ],
      "metadata": {
        "id": "mjPY_JlB1jDj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__ (self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, drop_rate):\n",
        "    super().__init__()\n",
        "    self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "    self.encoder = nn.Sequential(*[\n",
        "        TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)\n",
        "        for _ in range(depth)\n",
        "    ])\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_embed(x)\n",
        "    x = self.encoder(x)\n",
        "    x = self.norm(x)\n",
        "    cls_token = x[:, 0]\n",
        "    return self.head(cls_token)"
      ],
      "metadata": {
        "id": "BnOJCYqr1rdw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create an instance from our vision transformer class/ instantiate model"
      ],
      "metadata": {
        "id": "-lXzrDzr6hz3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(\n",
        "    IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES,\n",
        "    EMBED_DIM, DEPTH, NUM_HEADS, MLP_DIM, DROP_RATE\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "eraB2VKh6pqC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##9 Define a loss function and optimizer"
      ],
      "metadata": {
        "id": "Wm17iEU3-CMC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() #used when classifying into muliple classes. measures how wrong our model is\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), # updates our model parameters to try and reduce the loss\n",
        "                             lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "IpAScdVV-XpG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRJCMXxNYEVo",
        "outputId": "b22057d6-1aa1-4060-f433-51f1a4c9895a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KQJcNz-YcrQ",
        "outputId": "12911541-a7f6-4ede-f9bd-d696c0fbbed4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.0003\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to train our model\n",
        "# Defining a training loop function"
      ],
      "metadata": {
        "id": "GdiVXjKQYlPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion):\n",
        "  model.train()\n",
        "\n",
        "  total_loss, correct = 0, 0\n",
        "\n",
        "  for x, y in loader:\n",
        "    #moving/sending data into target device\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    #1 forward pass(model outputs raw logits)\n",
        "    out = model(x)\n",
        "    #2 calculate the loss per batch\n",
        "    loss = criterion(out, y)\n",
        "    #3 Perform back propagation to compute the gradient of the loss with respect to our parameters.\n",
        "    loss.backward()\n",
        "    #4 perform gradient descent algorithm\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item() * x.size(0)\n",
        "    correct += (out.argmax(1) == y).sum().item()\n",
        "    # you have to scale the loss(normalization step to make the loss general across all batches)\n",
        "  return total_loss / len(loader.dataset), correct / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "QB03rxdtY4Rb"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}