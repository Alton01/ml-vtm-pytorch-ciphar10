{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzafmlLSva9Cm1U/bBKFdT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alton01/ml-vtm-pytorch-ciphar10/blob/main/cifar_10VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouq7o88fPDTt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "id": "4dNCITNJRspE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision.__version__"
      ],
      "metadata": {
        "id": "8lLjdvdRR8hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. setup device agnostic code"
      ],
      "metadata": {
        "id": "ZixSqpMPSVDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "RMf9pCBoS_fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "xWzikhc9ScAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Using Device: {device}\")"
      ],
      "metadata": {
        "id": "xlXxjqC8ULv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Set the seed"
      ],
      "metadata": {
        "id": "2-TfVOX0UVHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "aRwG5IQKUe1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3e-4"
      ],
      "metadata": {
        "id": "gUw5PsxxVnQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Setting the hyperparameters"
      ],
      "metadata": {
        "id": "MRnaX-IcU8zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10 # try increasing epochs to 30\n",
        "LEARNING_RATE = 3e-4\n",
        "PATCH_SIZE = 4\n",
        "NUM_CLASSES = 10\n",
        "IMAGE_SIZE = 32 # transform image and make imagesize go to 224\n",
        "CHANNELS = 3\n",
        "EMBED_DIM = 256\n",
        "NUM_HEADS = 8 #  try increasing number of heads\n",
        "DEPTH = 6\n",
        "MLP_DIM = 512\n",
        "DROP_RATE = 0.1"
      ],
      "metadata": {
        "id": "pp0hwg78VIKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 5.Define image transformations operation"
      ],
      "metadata": {
        "id": "GLJxjpGaW88h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "    #helps the model to converge faster\n",
        "    # helps to make the numerical computation stable\n",
        "])\n",
        "\n",
        "#transform_train = transforms.Compose([\n",
        " #   transforms.RandomCrop(32, padding=4),\n",
        " #   transforms.RandomHorizontalFlip(),\n",
        "  #  transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "  #  transforms.ToTensor(),\n",
        "  #  transforms.Normalize((0.5), (0.5))\n",
        "    #helps the model to converge fasterand helps model perform better\n",
        "    # helps to make the numerical computation stable\n",
        "#])"
      ],
      "metadata": {
        "id": "9FSs0lBzXGXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6. Getting a dataset"
      ],
      "metadata": {
        "id": "sRd71HkaYN7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root=\"data\",\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=transform)"
      ],
      "metadata": {
        "id": "WJDeB54wYX7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.CIFAR10(root=\"data\",\n",
        "                                train=False,\n",
        "                                download=True,\n",
        "                                transform=transform)"
      ],
      "metadata": {
        "id": "7PZPOtq_gJXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "-0QkATx1gklm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "v9GV9ePkgwbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## step 7 converting our datasets into dataloaders\n",
        "## This means breaking dataset into batches or minibatches\n",
        "## Computationally efficient ascomputing hardware may not handle 50,000 images\n",
        "## At a go\n",
        "## so we break it into 128 images at a time (BATCH_SIZE = 128)\n",
        "## It gives our neural network more chances to update its gradient per epoch"
      ],
      "metadata": {
        "id": "Kf7pEZ2ag912"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=False)"
      ],
      "metadata": {
        "id": "-Z73ow7ZiDVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets check out what we have created\n",
        "print(f\"DataLoader: {train_loader, test_loader}\")\n",
        "print(f\"Length of train_loader: {len(train_loader)} batches of {BATCH_SIZE}...\")\n",
        "print(f\"Length of test_loader: {len(test_loader)} batches of {BATCH_SIZE}...\")"
      ],
      "metadata": {
        "id": "p_4YG462jxhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 8. Building Vision Transformer model from scratch.\n",
        "#Splitting each image into smaller bits(patches)"
      ],
      "metadata": {
        "id": "rCzMePiilEK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size,\n",
        "               patch_size,\n",
        "               in_channels,\n",
        "               embed_dim):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.proj = nn.Conv2d(in_channels=in_channels,\n",
        "                          out_channels=embed_dim,\n",
        "                          kernel_size=patch_size,\n",
        "                          stride=patch_size)\n",
        "    num_patches = (img_size // patch_size) ** 2\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    B = x.size(0)\n",
        "    x = self.proj(x)\n",
        "    x = x.flatten(2).transpose(1, 2)\n",
        "    cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "    x = x + self.pos_embed\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "A3YGZIN-lOE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define multi layer perceptron class"
      ],
      "metadata": {
        "id": "4DbPR77VrRDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_features,\n",
        "               hidden_features,\n",
        "               drop_rate):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features=in_features,\n",
        "                         out_features=hidden_features)\n",
        "    self.fc2 = nn.Linear(in_features=hidden_features,\n",
        "                         out_features=in_features)\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(F.gelu(self.fc1(x)))\n",
        "    x = self.dropout(self.fc2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "Rn-UE_FHrWXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Building a transformer encoder layer"
      ],
      "metadata": {
        "id": "b_Fd3bZ7xLw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, mlp_dim, drop_rate):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "yngDwS5cxQOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create a vision transformer class to combine all of these components"
      ],
      "metadata": {
        "id": "mjPY_JlB1jDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__ (self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, drop_rate):\n",
        "    super().__init__()\n",
        "    self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "    self.encoder = nn.Sequential(*[\n",
        "        TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)\n",
        "        for _ in range(depth)\n",
        "    ])\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_embed(x)\n",
        "    x = self.encoder(x)\n",
        "    x = self.norm(x)\n",
        "    cls_token = x[:, 0]\n",
        "    return self.head(cls_token)"
      ],
      "metadata": {
        "id": "BnOJCYqr1rdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create an instance from our vision transformer class/ instantiate model"
      ],
      "metadata": {
        "id": "-lXzrDzr6hz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(\n",
        "    IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES,\n",
        "    EMBED_DIM, DEPTH, NUM_HEADS, MLP_DIM, DROP_RATE\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "eraB2VKh6pqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##9 Define a loss function and optimizer"
      ],
      "metadata": {
        "id": "Wm17iEU3-CMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() #used when classifying into muliple classes. measures how wrong our model is\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), # updates our model parameters to try and reduce the loss\n",
        "                             lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "IpAScdVV-XpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion"
      ],
      "metadata": {
        "id": "aRJCMXxNYEVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "id": "0KQJcNz-YcrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to train our model\n",
        "# Defining a training loop function"
      ],
      "metadata": {
        "id": "GdiVXjKQYlPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "jKVpfzmkoobg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion):\n",
        "  model.train()\n",
        "\n",
        "  total_loss, correct = 0, 0\n",
        "\n",
        "  for x, y in loader:\n",
        "    #moving/sending data into target device\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    #1 forward pass(model outputs raw logits)\n",
        "    out = model(x)\n",
        "    #2 calculate the loss per batch\n",
        "    loss = criterion(out, y)\n",
        "    #3 Perform back propagation to compute the gradient of the loss with respect to our parameters.\n",
        "    loss.backward()\n",
        "    #4 perform gradient descent algorithm\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item() * x.size(0)\n",
        "    correct += (out.argmax(1) == y).sum().item()\n",
        "    # you have to scale the loss(normalization step to make the loss general across all batches)\n",
        "  return total_loss / len(loader.dataset), correct / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "QB03rxdtY4Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining evaluate function"
      ],
      "metadata": {
        "id": "4fDgki5thHU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "  model.eval() #eval sets the mode of the model into evaluation\n",
        "  correct = 0\n",
        "  with torch.inference_mode():\n",
        "    for x, y in loader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      out = model(x)\n",
        "      correct += (out.argmax(dim=1) == y).sum().item()\n",
        "  return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "xPBUHDNUhUQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS"
      ],
      "metadata": {
        "id": "W0QXEYggsaQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "vbqRCZEN7KHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## training the model"
      ],
      "metadata": {
        "id": "DmV6K1bhj3Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies, test_accuracies = [], []\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "  test_acc = evaluate(model, test_loader)\n",
        "  train_accuracies.append(train_acc)\n",
        "  test_accuracies.append(test_acc)\n",
        "  print(f\"Epoch: {epoch + 1}/{EPOCHS}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}%, Test acc: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "1ZHt_dCEs-Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies"
      ],
      "metadata": {
        "id": "6wNpHzE1HJBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracies"
      ],
      "metadata": {
        "id": "MiN_2yrAHdJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot accuracy\n",
        "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Test Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zZVl-JUPHiHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "AVDUWmbmKA9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_plot_grid(model,\n",
        "                          dataset,\n",
        "                          classes,\n",
        "                          grid_size=3):\n",
        "  model.eval()\n",
        "  fig, axes = plt.subplots(grid_size, grid_size, figsize=(9, 9))\n",
        "  for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "      idx = random.randint(0, len(dataset) -1)\n",
        "      img, true_label = dataset[idx]\n",
        "      input_tensor = img.unsqueeze(dim=0).to(device)\n",
        "      with torch.inference_mode():\n",
        "        output = model(input_tensor)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "      img = img / 2 + 0.5 #unnormalize our images to be able to plot them with matplotlib\n",
        "      npimg = img.cpu().numpy()\n",
        "      axes[i, j].imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "      truth = classes[true_label] == classes[predicted.item()]\n",
        "      if truth:\n",
        "        color = \"g\"\n",
        "      else:\n",
        "        color = \"r\"\n",
        "\n",
        "      axes[i, j].set_title(f\"Truth: {classes[true_label]}\\n, Predicted: {classes[predicted.item()]}\", fontsize=10, c=color)\n",
        "      axes[i, j].axis(\"off\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "NubwaRDEKvkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}